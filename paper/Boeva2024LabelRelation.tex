\documentclass[a4paper, 12pt]{article} %{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{multirow}
\usepackage{subcaption}
 

\newcommand{\vecE}{\mathbf{e}}
\newcommand{\vecX}{\mathbf{x}}
\newcommand{\vecY}{\mathbf{y}}


\renewcommand{\abstractname}{Аннотация}


\title{Label Attention Network для последовательной классификации по нескольким меткам.}

\author{ Боева Галина\\
	Антиплагиат\\
	Сколтех\\ 
	\texttt{boeva.gl@phystech.edu} 
	\AND
        Консультант: к.ф.-м.н. Грабовой Андрей\\
	Антиплагиат\\
	\texttt{grabovoy.av@phystech.edu} 
        \AND
        Эксперт: к.ф.-м.н. Зайцев Алексей\\
	Сколтех\\
	\texttt{a.zaytsev@skoltech.ru}
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{\today}

%\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\begin{lstlisting}[language=R]
%chunk = 1
<<echo=FALSE,results=tex>>=
cat("dataset","\n")
cat("\\textbf{dataset}","\n")   # it should be bold
@
\end{lstlisting}
\maketitle

\begin{abstract}
Рассматривается задача прогнозирования временных наборов для последовательных данных. Современные подходы фокусируются на архитектуре преобразования последовательных данных, используя собственного внимания (``self-attention'') к элементам в последовательности. В этом случае учитывается временные взаимодействия событий, но теряем информацию о взаимозависимостях меток. Мотивированные этим недостатком, предлагается использовать механизм собственного внимания(``self-attention'') к меткам, предшествующим прогнозируемому шагу. Поскольку рассматриваемый подход представляет собой сеть внимания к меткам --- LANET. В данном исследовании обосновывается вывод причинно-следственной связи внимания, которое указывает на важность меток и их взаимозависимости.
\end{abstract}


\keywords{временные ряды \and взаимосвязь меток}

\section{Введение}
Классификация с несколькими метками является более естественной, чем бинарная или многоклассовая классификация, поскольку все, что окружает нас в реальном мире, обычно описывается несколькими метками~\cite{liu2021emerging}. Та же логика может быть перенесена на последовательность событий с отметками времени. События в последовательности, как правило, характеризуются несколькими категориальными значениями вместо одной. Существует множество подходов к классификации с несколькими метками в компьютерном зрении ~\cite{durand2019learning}, обработке естественного языка ~\cite{xiao2019label} или классической структуре табличных данных ~\cite{tarekegn2021review}. Однако постановке задачи с несколькими метками для последовательностей событий, как правило, уделяется меньше внимания. Итак, основной целью является противостоять такому недостатку внимания и решить проблему предсказания набора меток для последовательных данных с временными метками.  
\begin{figure*}
    \centering
    \includegraphics[scale=0.75]{images/nir_inrto.pdf}
    \caption{На рисунке показано визуальное представление постановки задачи. Наша модель должна предсказать метки для момента времени $t_4$, учитывая историю предыдущих наборов меток. Требуется предсказать несколько меток, так что это определение задачи классификации с несколькими метками.}
    \label{fig:nir-intro}
\end{figure*}
Важно отметить, что модель должна предсказывать набор меток, соответствующих следующему шагу, принимая во внимание содержимое предыдущих групп меток для последовательности событий, связанных с объектом(Рисунок~\ref{fig:nir-intro}).

Взаимодействие между состояниями объекта в разные временные метки имеет важное значение для решения задач с последовательными данными~\cite{hartvigsen2020recurrent}. Следовательно, выразительные и мощные модели должны быть способны изучать такие взаимодействия. Несколько архитектур нейронных сетей, таких как трансформеры~\cite{vaswani2017attention} или рекуррентные нейронные сети~\cite{grossberg2013recurrent}, способны делать это. Например, трансформер напрямую определяет механизм внимания, который измеряет, как связаны различные временные метки в последовательности. Однако применение современных методов глубокого обучения ограничено~\cite{zhang2020multi}, и они в первую очередь сосредоточены на прогнозировании меток для последовательности в целом. 

\subsection{Основные подходы для задачи классификации с несколькими метками.}
Постановка задачи классификации с несколькими метками возникает во многих различных областях, например, при категоризации текста или тегировании изображений, и все они влекут за собой свои собственные особенности и проблемы. В обзоре~\cite{zhang2013review} исследуются основы обучения с использованием нескольких меток, обсуждаются хорошо зарекомендовавшие себя методы, а также самые последние подходы. Возникающие тенденции рассматриваются в более свежем обзоре ~\cite{liu2021emerging}.

В работе~\cite{shou2023concurrent} рассматривается та же постановка задачи классификации с несколькими метками в потоке событий, что и у нас. Модель авторов нацелена на фиксацию временных и вероятностных зависимостей между типами параллельных событий путем кодирования исторической информации с помощью энкодера, а затем использования условной смеси экспертов Бернулли. В этой статье~\cite{yu2023continuous} обсуждается постановка задачи прогнозирования временных наборов для пользователей, она предлагает систему непрерывного обучения, которая позволяет явно фиксировать изменяющиеся пользовательские предпочтения, поддерживая банк памяти, который мог бы хранить состояния всех пользователей и элементов. В этой парадигме авторы строят неубывающую универсальную последовательность, содержащую все пользовательские взаимодействия, а затем в хронологическом порядке извлекают уроки из каждого взаимодействия. Для исследования взаимосвязи между продуктами в корзине был предложен ConvTSP~\cite{zhang2023conv}, который объединяет динамические интересы пользователей и статистические интересы в единое векторное представление.

\subsection{Рекомендательные системы.}
В этом разделе мы представим статьи, связанные с проблемой рекомендации следующей корзины. Эта формулировка похожа на нашу, поэтому мы также рассмотрели многие подходы и идеи при анализе нашей области исследований. Авторы в~\cite{ariannezhad2023personalized} предложили персонализированную модель, которая фиксирует краткосрочные зависимости внутри временного набора продуктов, а также долгосрочную, основанную на исторической информации о пользователях. Также в~\cite{yannam2023hybrid} для соединения локальной и глобальной пользовательской информации предлагается гибридный метод, основанный на автоэнкодере~\cite{liou2014autoencoder} для извлечения контекста и рекуррентные нейронные сети для понимания динамики изменения интересов. Чтобы преодолеть подобные проблемы, для предсказания следующей рекомендации создается сеть внимания на основе графов, использующая hyper-edge подход~\cite{song2023hgat}.
При такой постановке задачи возникает сложность работы со словарем товарных категорий, поскольку они насчитывают тысячи значений, в исследовании~\cite{van2023next} используется GRU для прогнозирования следующей корзины, которая легко масштабируется до большого ассортимента.


\paragraph{\textbf{Вклад.}}
Разработана архитектура на основе трансформера на основе собственного внимания между метками для работы над задачей классификации последовательностей событий по нескольким меткам. Основной вклад заключается в следующем:
\begin{itemize}
    \item Была введена архитектура LANET для прогнозирования набора меток для текущего события, используя информацию из предыдущих событий. Особенностью архитектуры является вычисление собственного внимания между представлениями меток.  
    \item LANET превосходит модели на основе трансформера, которые фокусируются на вычислении собственного внимания между временными метками. Оцениваются все показатели в различных наборах данных(будет дополнение).
\end{itemize}


\section{Постановка задачи}
% Рассматривается задача классификации с несколькими метками для последовательности $S = \{(\bf{X}_{i}, \bf{Y}_{i})\}_{i = 1}^{t-1}$. Он состоит из набора меток $\bf{Y}_{i} \in R^m$, где $m$ --- количество меток в наборе, и набора признаков $\bf{X}_{i}\in R^n$, где $n$ --- количество признаков, специфичных для каждой временной метки от $1$ до $t-1$. Индекс соответствует времени события, поэтому $(bf{X}_{1}, bf{X}_{1})$ --- это информация о первом событии, а $(bf{X}_{t-1}, bf{X}_{t-1})$ - это информация о последнем наблюдаемом событии.
% Множество $Y_i \subseteq \mathcal{Y}$, где $\mathcal{Y} = \{1, 2, \dots, K\}$ --- это множество всех возможных меток. Установленный размер $X_{i}$ равен размеру $Y_{i}$. Каждая метка из $Y_{i}$ сопровождается числовым или категориальным признаком из $X_{i}$ в соответствующей позиции.

% Также может быть дополнительный вектор признаков $\mathbf{z}$, описывающий рассматриваемую последовательность $S$ в целом, например, идентификатор пользователя.
% Цель последовательной классификации с несколькими метками состоит в том, чтобы предсказать набор меток $Y_{t}$ для следующей временной метки.

% Введем функцию $f(\cdot) \in [0, 1]^K$, которая принимает историческую информацию о событиях в качестве входных данных и выводит вектор оценок для каждой из меток $K$. Эти оценки представляют собой вероятности присутствия метки в следующем наборе, связанных с событием.

% В нашей настройке есть ограничение на размер прошлого, доступного модели.
% $S^t = \{(X_{j}, Y_{j})\}_{j = t - \tau}^{t-1}$, где $\tau$ означает количество событий, предшествующих рассматриваемому событию, с отметкой времени $t$, которая равна приписывается целевому набору меток $Y_{t}$.
% Итак, более формально $f(\cdot)$ имеет вид:
% $$
% f(X_{t - \tau}, \ldots, X_{t-1}, Y_{t - \tau}, \ldots, Y_{t-1}, \bf{z}): R^[0, 1]^K
% $$
% для предсказания $Y_{t}$.
Задача классификации с несколькими метками это задача изучения функции f, которая сопоставляет входные данные с подмножествами набора меток $\mathcal{L} = \{1, 2, \dots , L\}$. Рассмотрим набор из $N$ последовательностей $D = {(\mathbf{X}_n, \mathbf{Y}_n)}^{N}_{n=1}$. Предполагается, что пары $(\mathbf{X}_n, \mathbf{Y}_n)$ являются случайными величинами, которые подчиняются неизвестному распределению $P(\mathbf{X}, \mathbf{Y})$.Пусть $T_n = |\mathbf{Y}_n|$ обозначает
размер набора меток, связанного с $\mathbf{X}_n$. В нашей постановке есть ограничение на размер прошлого, доступного модели. Обозначим $\tau$ означает количество событий, предшествующих рассматриваемому событию, тогда $D^u_t = {(\mathbf{X}^u_j, \mathbf{Y}^u_j)}_{j = t - \tau}^{t-1}$, следующих одному и тому же неизвестному распределению $P(\mathbf{X}, \mathbf{Y})$. Введем функцию $f(\cdot) \in [0, 1]^K$, которая принимает историческую информацию о событиях в качестве входных данных и выводит вектор оценок для каждой из меток $K$. Эти оценки представляют собой вероятности присутствия метки в следующем наборе, связанных с событием. Итак, более формально $f(\cdot)$ имеет вид:
$$
f: D^u_t \to [0, 1]^K 
$$
для предсказания $Y_{t}.$
Цель изучения f - минимизировать ожидаемую потерю на подвыборке в соответствии с базовым распределением P:
$$\displaystyle\min_f \mathbb{E}_{(\mathbf{X},\mathbf{Y})\sim P} [Loss(\mathbf{Y}, f(\mathbf{X}))],$$ где $Loss$ --- это логарифмическая функция потерь.

Чтобы завершить прогноз, нам нужна отдельная модель принятия решений о метках $g(f(\cdot))$, которая преобразует доверительные баллы в метки.
Например, можно сравнить оценку для $k$-й метки с выбранным пороговым значением $\beta_k$: если $f_k(\cdot) > \beta_k, k = 1, \dots, K$, то модель предсказывает, что $k$-я метка присутствует. Таким образом, модель $g$ создает набор меток $\hat{Y}_{t} \subseteq \mathcal{Y}$ на основе входных оценок достоверности.  

\section{Архитектура LANET}
Большинство моделей, связанных с трансформаторами, используемых для последовательного предсказания с несколькими метками, используют вычисление собственного внимания между последовательными представлениями входных временных меток. 
Вместо этого LANET использует собственное внимание между представлениями меток. 
Итак, у него есть входные данные, состоящие из $K$ векторов.
Ниже описывается, как агрегировать последовательность векторов размером от $\tau$ до $K$ с помощью \textbf{Слоя векторных представлений}. Затем определяется \textbf{Слой собственного внимания}.
Чтобы получить предсказания, применяется \textbf{Слой предсказания}.

\paragraph{\textbf{Слой векторных представлений.}} Используется следующий подход для использования различных частей входных данных для последовательностей событий с несколькими метками:
\begin{itemize}
    \item \textbf{\emph{Векторное представление идентификаторов:}} Для идентификаторов изучается матрица эмбеддингов;
    \item \textbf{\emph{Векторное представление времени:}} Для каждой временной метки мы знаем значение $dt$, которое равно разнице в днях между рассматриваемой и предыдущей временной меткой. Проводим обучение представлений для каждого наблюдаемого значения $dt$. Также учитывается порядок событий, поэтому мы смотрим на предстваления для позиций: $1, \dots,$ \textit{$\tau$}, чтобы добавить их к $dt$ представлению соответствующей временной метки; 
    \item \textbf{\emph{Представление суммы:}} Все суммы преобразуются в ячейки, разбивая непрерывные суммы сумм на интервалы. Каждому интервалу присваивается уникальный номер. Затем для каждого уникального номера создается представление.
    \item \textbf{\emph{Transformer label encoder:}} Для построения входных данных LANET используются данные, связанные с определенным идентификатором для временных меток \textit{$\tau$}. Дальше идет объединение представлений меток, которые встречаются во время временных меток \textit{$\tau$}, с соответствующими векторами времени и суммы. Если метка не принадлежит истории последних временных меток \textit{$\tau$}, к ней добавляется векторы нулей в виде представлений времени и суммы. Если конкретная метка встречается несколько раз на предыдущих шагах \textit{$\tau$}, то создаются временные представления для каждого отдельного вхождения, а затем суммируются их, чтобы получить окончательное представление времени для этой метки. То же самое происходит при построении вектора суммы в ситуации повторения метки.
\end{itemize}

Итак, в результате слоя вложения у нас есть $K + 1$ векторов вложения. 
Первый вектор $\vecE_0$ соответствует вложениям общих (ID) признаков для последовательности.
Все остальные векторы $\vecE_k, k = 1, \dots, K$ являются объединением вложения метки с соответствующими вложениями времени и суммы. Оказывается, что векторы для исторически не задействованных меток --- это просто вложения меток, поскольку мы суммируем их с нулевыми векторами времени и количества просмотров.
При обучении всех весов встраивания они инициализируются из нормального распределения $\mathcal{N}(0, 1)$ и затем оптимизируются.
% Все остальные векторы являются конкатенациями вложения метки из словаря и означают объединения сумм и временных меток для конкретной метки.  

\paragraph{\textbf{Собственное внимание.}} После получения представлений из наших данных давайте перейдем к компоновке нашей архитектуры. 
Пусть $E = \{\vecE_0, \dots, \vecE_{K}\}$, $\vecE_i \in \mathbb{R}^d$ - последовательность входных представлений в энкодер, где $\vecE_0$ соответствует представлению $\mathbf{z}$ и все остальные $\vecE_i$ соответствуют представлениям, фиксирующим историческую информацию с точки зрения метки. 
В архитектуре Transformer влияние представление $\vecE_j$ на представление $\vecE_i$ достигается с помощью собственного внимания. Вес внимания $\alpha_{ij}$ и обновленный вектор $\vecE_i^{\prime}$ рассчитываются как:
$$ \vecE_i^{\prime} = \sum_{j = 0}^{K} \alpha_{ij} (W^v \vecE_j);\hspace{3 мм} \alpha_{ij} = \mathrm {softmax}\left ( (W ^ q \vecE_i)^T (W^k \vecE_j) / \sqrt{d} \right),$$ где $W^k$ --- весовая матрица ключа, $W^q$ - весовая матрица запроса, а $W^v$ - весовая матрица значения. Такую процедуру внедрения обновлений можно повторить несколько раз, указав количество уровней энкодера.
% Поверх подуровня самообучения мы применяем обычные процедуры уровня преобразователя-кодировщика~\cite{transformer_encoder_torch}.
% В частности, мы выполняем TODO.


\paragraph{\textbf{Уровень прогнозирования.}} Обновленные представления обрабатываются одним полносвязным слоем, чтобы получить окончательные представления для каждой метки $\{\vecE^{(final)}_j\}_{j = 1}^K$.
Таким образом, мы получаем $\{f_j\}_{j = 1}^K$, которые используются в классификаторе с несколькими метками с пороговым значением $t_j$, выбранным отдельно для каждой метки с использованием валидационной выборки.


\section{Планирование эксперимента}

\subsection{Описание данных}

\begin{table}[t!]
\centering
% \begin{tabular}{ccccccc}
\begin{tabular}{p{0.3cm}p{2.2cm}p{1.7cm}p{1.6cm}p{1.6cm}p{1.8cm}p{1.1cm}}
\hline
& Dataset & \# events & Median & Max & \# unique & Diff \\
& &  & set size & set size &  labels \\
\hline
& Sales & 47 217 & 16 & 48  & 84 & 0.0632 \\
& Demand & 5 912 & 13 & 24  & 33 & 0.0957 \\
\hline
\end{tabular}
\caption{Характеристики наборов данных, используемых в задачах последовательной классификации с несколькими метками.}
\label{tab:multi_datasets}
\end{table}

\textbf{Набор данных о продажах}~\cite{sales} - это исторические данные о продажах в разных магазинах. Метки относятся к категориям товаров, а сумма ---- это количество проданных товаров для определенной категории. 

\textbf{Набор данных о спросе}~\cite{demand} описывает исторический спрос на продукцию нескольких складов. Функция метки означает категорию продукта, а функция количества относится к соответствующему спросу.

Общая статистика по рассмотренным наборам данных приведена в таблице~\ref{tab:multi_datasets}.

Мы представляем количество наблюдаемых событий, средний размер набора всех доступных наборов меток $\mathrm{median}({|Y_{ij}|}_{i, j = 1, 1}^{n, t_i})$, максимальный размер набора меток, который встречается в набор данных $\max({|Y_{ij}|}_{i, j = 1, 1}^{n, t_i})$, количество уникальных меток $K$ и \textit{Diff}. 
\textit{Diff} измеряет дисбаланс меток: мы вычисляем 5$\%$ и 95$\%$ квантилей для частот меток и берем разницу между ними.
Представление меток не сбалансировано в большинстве наборов данных, но мы используем метрики, которые учитывают этот эффект.

\section{Метрики} 

В статье~\cite{wu2017unified} представлен обзор показателей для классификации по нескольким меткам. Мы рассматриваем наиболее часто используемые и всеобъемлющие показатели для оценки качества классификации с использованием нескольких меток.

Каждому образцу $\mathbf{X}_i$ присваивается  набор $\mathbf{Y}_i$. Мы можем построить вектор $\mathbf{y}_{i}$ из $\mathbf{Y}_i$ таким образом, что $y_ {ij} = 1$ ($i \in \{1, \dots, N\}, j \in \{1, \dots, K\}$) соответствует случай, когда $j$-я метка релевантна для $i$-го экземпляра и $y_{ij} = 0$ в случае нерелевантности. Поскольку модель $g$ предоставляет окончательный набор меток $\hat{\mathbf{Y}}_i$ из оценок достоверности $\mathbf{f}_i$ ($i \in \{1, \dots, N\}$), давайте обозначим вектор прогнозируемых меток как $\mathbf{g}_i$. Элемент $g_{ij}$ равен $1$, если сеть прогнозирует $j$-ю метку как соответствующую для $i$-го экземпляра, и равен $0$ в противном случае.

\begin{itemize} 
    \item \textbf{Micro-F1}: 
    
    $$ 
    micro\textrm{--}F1 = \frac{2\sum_{j=1}^{K}\sum_{i=1}^{N}y_{ij}g_{ij}}{\sum_{j=1}^{K}\sum_{i=1}^{N}(y_{ij} + g_{ij})}. 
    $$
    
    \item \textbf{Macro-F1}: 
    
    $$ 
    macro\textrm{--}F1 = \frac{1}{K}\sum_{j=1}^{K} \frac{2\sum_{i=1}^{N}y_{ij}g_{ij}}{\sum_{i=1}^{N}(y_{ij} + g_{ij})}.
    $$
    
    \item \textbf{Micro-AUC}:  
    
    $$
    micro\textrm{--}AUC = \frac{|W_{micro}|}{(\sum_{i=1}^{N}|Y_{i\cdot}^{+}|)(\sum_{i=1}^{N}|Y_{i\cdot}^{-}|)}, 
    $$
    
    $$ W_{micro} = \{(a, b, i, j) | (a, b) \in Y_{\cdot i}^{+} \times Y_{\cdot j}^{-}, f_i(x_a) \geq f_j(x_b)\}, $$ где  $Y_{i\cdot}^{+} = \{j | y_{ij}=1\}$ и $Y_{i\cdot}^{-} = \{j | y_{ij}=0\}$ обозначают релевантные и нерелевантные метки для $i$-го экземпляра соответственно. $Y_{\cdot j}^{+} = \{i | y_{ij}=1\}$ и $Y_{\cdot j}^{-} = \{i | y_{ij}=0\}$ обозначают наборы положительных и отрицательных экземпляров $j$-я метка.
    
    \item \textbf{Macro-AUC}: 
    
    $$
    macro\textrm{--}AUC = \frac{1}{K}\sum_{j=1}^{K}\frac{W_{macro}^{j}}{|Y_{\cdot j}^{+}||Y_{\cdot j}^{-}|}
    $$
    
    $$ W_{macro}^{j} = \{ (a, b) \in Y_{\cdot j}^{+} \times Y_{\cdot j}^{-} | f_{j}(x_a) \geq f_{j}(x_b)\} $$
\end{itemize}

 

\section{Основной эксперимент}
Эксперимент проводится на двух выборках Sales и Demand. Данные выборки представлены в таблице~\ref{tab:multi_datasets}. Основным результатом будет сравнение подхода  с собственным вниманием между представлениями на основе агрегации по меткам классов и подходом, использующим временную агрегацию для представлений. Также будут представлены базовые подходы, работающие с временными рядами это градиентный бустинг и LSTM. Сравнение представлено в Таблице~\ref{table:diff_data}.  

\begin{table*}[t] % table
\centering
\small
\begin{tabular}{ccccccc}
\hline
Dataset & Model & Micro-AUC$\uparrow$ & Macro-AUC$\uparrow$ & Micro-F1$\uparrow$ & Macro-F1$\uparrow$\\
\hline
& LSTM & \underline{0.8670} & \underline{0.7346} & \underline{0.5600} & \underline{0.4389} \\
& TransformerBase     & 0.8604 & 0.7206 & 0.5348 & 0.3751 \\
% & Gradient Boosting   & 0.7807  & 0.6156 & 0.4275 & 0.4151\\
& CLASS2C2AE          & 0.8528 & 0.6881 & 0.5116 & 0.4217 \\
\multirow{-5}{*}{Sales} & LANET (ours) & \textbf{0.9069} & \textbf{0.7627} & \textbf{0.6235} & \textbf{0.4901} \\ \hline

& LSTM & \textbf{0.8829} & \textbf{0.7633} & \underline{0.6746} & \textbf{0.5929} \\
& TransformerBase & 0.8624  & 0.7240 & 0.6491 & 0.5678 \\
% & Gradient Boosting & 0.7784  & 0.6219  & 0.6010 & 0.5466\\
& CLASS2C2AE & 0.8342  & 0.7079 & 0.6738 & 0.5581\\
\multirow{-5}{*}{Demand} & LANET (ours) & \underline{0.8806} & \underline{0.7373} & \textbf{0.7038} & \underline{0.5908} \\ \hline


\end{tabular}
\centering
\caption{Сравнение нашего метода LANET с базовыми данными по двум различным наборам данных. Выделены лучшие значения, а значения вторые по рангу подчеркнуты.
}
\label{table:diff_data}
\end{table*}

Стоит изучить, какой подход к вычислению собственного внимания более важен: между метками или между временными метками.
Таблица~\ref{table:attents} отвечает на этот вопрос.
\begin{table}[ht!]
\centering
\small
\begin{tabular}{lcccccc}
\hline
 & Micro-AUC$\uparrow$ & Macro-AUC$\uparrow$ & Micro-F1$\uparrow$ & Macro-F1$\uparrow$\\ 
\hline
Label-attention & \underline{0.881} $\pm$ 0.007 & \underline{0.737} $\pm$ 0.017 & \underline{0.704} $\pm$ 0.018 & \textbf{0.591} $\pm$ 0.003\\
Time-attention & 0.837 $\pm$ 0.004 & 0.682 $\pm$ 0.007 & 0.674 $\pm$ 0.009  & 0.588 $\pm$ 0.002 \\
Concat-attention & 0.835 $\pm$ 0.001 & 0.681 $\pm$ 0.010 & 0.666 $\pm$ 0.000 & 0.587 $\pm$ 0.001 \\
Gated-attention & 0.829 $\pm$ 0.030 & 0.668 $\pm$ 0.048 & 0.672 $\pm$ 0.010 & 0.578 $\pm$ 0.011 \\
\hline
 Absence-indication & \textbf{0.882} $\pm$ 0.004 & \textbf{0.742} $\pm$ 0.003 & \textbf{0.704} $\pm$ 0.010 & \underline{0.589} $\pm$ 0.004 \\
\hline
\end{tabular}
\centering
\caption{Сравнение показателей при расчете посещаемости между различными видами в LANET. }
\label{table:attents}
\end{table}
Label-attention - это наша базовая реализация. Time-attention - это случай, когда мы учитываем внимание только между просмотрами временных меток. Concat-attention подразумевает получение оценок достоверности путем объединения label-attention и time-attention. Мы узнаем коэффициенты важности двух форм внимания и используем их в качестве весов при суммировании просмотров внимания в случае gated-attention. Эксперимент с индикацией отсутствия заключается в добавлении обучаемого вектора к входным вложениям, если конкретная метка не участвует в рассматриваемой истории.

Важным параметром нашей работы является ограничение исторической информации с помощью параметра $\tau$. В нашей работе мы концентрируемся на событиях с временными метками. Модель учитывает только последние временные метки для прогнозирования. Таким образом, возникает естественный вопрос, как качество модели зависит от количества временных меток, используемых для построения прогноза. Длина входной последовательности для модели равна параметру \textit{look\_back}. Зависимость показателя micro-AUC от \textit{look\_back} представлена в~\ref{tau}



\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=0.9\linewidth]{images/tau.pdf}
         \caption{Зависимость micro-AUC от параметра \textit{$\tau$}.}
         \label{fig:look_back}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.47\textwidth}
         \centering
         \includegraphics[width=0.9\linewidth]{images/emb.pdf}
         \caption{Зависимость micro-AUC от размера представлений.}
         \label{fig:emb_dim}
     \end{subfigure}
    \caption{Зависимость показателей от параметров модели для набора данных о спросе.}
    \label{tau}
\end{figure}
 
\bibliographystyle{unsrt}
\bibliography{references}

\section{Выводы}
Была рассмотрена задача последовательной классификации с несколькими метками.
Для решения этой задачи мы предлагаем архитектуру LANET. В ней исследуется связь между предыдущими векторами с несколькими метками. В частности, LANET использует собственное внимание между метками, чтобы способствовать выявлению исторических взаимозависимостей между ними. 
LANET демонстрирует наилучшие показатели по двум рассмотренным наборах данных.  
Однако нынешний подход ограничен случаем, когда для прогнозирования не требуется долговременная память, поскольку мы объединяем только несколько недавних временных меток и общие представления в конкретную последовательность.

\end{document}
