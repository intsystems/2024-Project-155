\documentclass[a4paper, 12pt]{article} %{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\newcommand{\vecE}{\mathbf{e}}
\newcommand{\vecX}{\mathbf{x}}
\newcommand{\vecY}{\mathbf{y}}
\renewcommand{\abstractname}{Аннотация}


\title{Выявление взаимозависимости между метками с использованием алгоритма, основанного на собственном внимании в задаче классификации с несколькими метками.}

\author{ Боева Галина\\
	Антиплагиат\\
	Сколтех\\ 
	\texttt{boeva.gl@phystech.edu} 
	\AND
        Консультант: к.ф.-м.н. Грабовой Андрей\\
	Антиплагиат\\
	\texttt{grabovoy.av@phystech.edu} 
        \AND
        Эксперт: к.ф.-м.н. Зайцев Алексей\\
	Сколтех\\
	\texttt{a.zaytsev@skoltech.ru}
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{\today}

%\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Большая часть доступной пользовательской информации может быть представлена в виде последовательности событий с временными метками. Каждому событию присваивается набор категориальных меток, будущая структура которых представляет большой интерес. Это задача прогнозирования временных наборов для последовательных данных. Современные подходы фокусируются на архитектуре преобразования последовательных данных, используя собственного внимания(``self-attention'') к элементам в последовательности. В этом случае мы учитываем временные взаимодействия событий, но теряем информацию о взаимозависимостях меток. Мотивированные этим недостатком, мы предлагаем использовать механизм собственного внимания(``self-attention'') к меткам, предшествующим прогнозируемому шагу. Поскольку наш подход представляет собой сеть внимания к меткам, мы называем ее LANET. Мы также обосновываем этот метод агрегирования, он положительно влияет на интенсивность события, предполагая, что мы используем стандартный вид интенсивности, предполагая работу с базовым процессом Хоукса.
\end{abstract}


\keywords{временные ряды \and взаимосвязь меток}

\section{Введение}
Классификация с несколькими метками является более естественной, чем бинарная или многоклассовая классификация, поскольку все, что окружает нас в реальном мире, обычно описывается несколькими метками~\cite{liu2021emerging}. Та же логика может быть перенесена на последовательность событий с отметками времени. События в последовательности, как правило, характеризуются несколькими категориальными значениями вместо одной. Существует множество подходов к классификации с несколькими метками в компьютерном зрении ~\cite{durand2019learning}, обработке естественного языка ~\cite{xiao2019label} или классической структуре табличных данных ~\cite{tarekegn2021review}. Однако постановке задачи с несколькими метками для последовательностей событий, как правило, уделяется меньше внимания. Итак, мы стремимся противостоять такому недостатку внимания и решить проблему предсказания набора меток для последовательных данных с временными метками.  
\begin{figure*}
    \centering
    \includegraphics[scale=0.75]{images/nir_inrto.pdf}
    \caption{На рисунке показано визуальное представление постановки задачи. Наша модель должна предсказать метки для момента времени $t_4$, учитывая историю предыдущих наборов меток. Требуется предсказать несколько меток, так что это определение задачи классификации с несколькими метками.}
    \label{fig:nir-intro}
\end{figure*}
Важно отметить, что модель должна предсказывать набор меток, соответствующих следующему шагу, принимая во внимание содержимое предыдущих групп меток для последовательности событий, связанных с объектом(Рисунок~\ref{fig:nir-intro}).

Взаимодействие между состояниями объекта в разные временные метки имеет важное значение для решения задач с последовательными данными~\cite{hartvigsen2020recurrent}. Следовательно, выразительные и мощные модели должны быть способны изучать такие взаимодействия. Несколько архитектур нейронных сетей, таких как трансформеры или рекуррентные нейронные сети, способны делать это. Например, трансформер напрямую определяет механизм внимания, который измеряет, как связаны различные временные метки в последовательности. Однако применение современных методов глубокого обучения ограничено~\cite{zhang2020multi}, и они в первую очередь сосредоточены на прогнозировании меток для последовательности в целом. 

\paragraph{\textbf{Основные подходы для задачи классификации с несколькими метками.}}
Постановка задачи классификации с несколькими метками возникает во многих различных областях, например, при категоризации текста или тегировании изображений, и все они влекут за собой свои собственные особенности и проблемы. В обзоре~\cite{zhang2013review} исследуются основы обучения с использованием нескольких меток, обсуждаются хорошо зарекомендовавшие себя методы, а также самые последние подходы. Возникающие тенденции рассматриваются в более свежем обзоре ~\cite{liu2021emerging}.

В работе~\cite{shou2023concurrent} рассматривается та же постановка задачи классификации с несколькими метками в потоке событий, что и у нас. Модель авторов нацелена на фиксацию временных и вероятностных зависимостей между типами параллельных событий путем кодирования исторической информации с помощью энкодера, а затем использования условной смеси экспертов Бернулли. В этой статье~\cite{yu2023continuous} обсуждается постановка задачи прогнозирования временных наборов для пользователей, она предлагает систему непрерывного обучения, которая позволяет явно фиксировать изменяющиеся пользовательские предпочтения, поддерживая банк памяти, который мог бы хранить состояния всех пользователей и элементов. В этой парадигме авторы строят неубывающую универсальную последовательность, содержащую все пользовательские взаимодействия, а затем в хронологическом порядке извлекают уроки из каждого взаимодействия. Для исследования взаимосвязи между продуктами в корзине был предложен ConvTSP~\cite{zhang2023conv}, который объединяет динамические интересы пользователей и статистические интересы в единое векторное представление.

\paragraph{\textbf{Рекомендательные системы.}}
В этом разделе мы представим статьи, связанные с проблемой рекомендации следующей корзины. Эта формулировка похожа на нашу, поэтому мы также рассмотрели многие подходы и идеи при анализе нашей области исследований. Авторы в ~\cite{ariannezhad2023personalized} предложили персонализированную модель, которая фиксирует краткосрочные зависимости внутри временного набора продуктов, а также долгосрочную, основанную на исторической информации о пользователях. Также в ~\cite{yannam2023hybrid} для соединения локальной и глобальной пользовательской информации предлагается гибридный метод, основанный на автоэнкодере для извлечения контекста и RNN для понимания динамики изменения интересов. Чтобы преодолеть подобные проблемы, для предсказания следующей рекомендации создается сеть внимания на основе графов, использующая hyper-edge подход~\cite{song2023hgat}.
При такой постановке задачи возникает сложность работы со словарем товарных категорий, поскольку они насчитывают тысячи значений, ~\cite{van2023next} использует GRU для прогнозирования следующей корзины, которая легко масштабируется до большого ассортимента.


\paragraph{\textbf{Вклад.}}
Мы разработали архитектуру на основе трансформера на основе собственного внимания между метками для работы над задачей классификации последовательностей событий по нескольким меткам. Наш основной вклад заключается в следующем:
\begin{itemize}
    \item Мы вводим архитектуру LANET для прогнозирования набора меток для текущего события, используя информацию из предыдущих событий. Особенностью архитектуры является вычисление собственного внимания между представлениями меток.  
    \item LANET превосходит модели на основе трансформера, которые фокусируются на вычислении собственного внимания между временными метками. Мы оцениваем все показатели в различных наборах данных(будет дополнение).
\end{itemize}


\section{Постановка задачи}
Мы рассмотрим классификацию с несколькими метками для последовательности $S = \{(X_{i}, Y_{i})\}_{i = 1}^{t-1}$. Он состоит из набора меток $Y_i$ и набора признаков $X_{i}$, специфичных для каждой временной метки от $1$ до $t-1$. Индекс соответствует времени события, поэтому $(X_{1}, Y_{1})$ - это информация о первом событии, а $(X_{t-1}, Y_{t-1})$ - это информация о последнем наблюдаемом событии.
Множество $Y_i \subseteq \mathcal{Y}$, где $\mathcal{Y} = \{1, 2, \dots, K\}$ - это множество всех возможных меток. Установленный размер $X_{i}$ равен размеру $Y_{i}$. Каждая метка из $Y_{i}$ сопровождается числовым или категориальным признаком из $X_{i}$ в соответствующей позиции.

У нас также может быть дополнительный вектор признаков $\mathbf{z}$, описывающий рассматриваемую последовательность $S$ в целом, например, идентификатор пользователя.
Цель последовательной классификации с несколькими метками состоит в том, чтобы предсказать набор меток $Y_{t}$ для следующей временной метки.

Мы создаем функцию $f(\cdot) \in [0, 1]^K$, которая принимает историческую информацию о событиях в качестве входных данных и выводит вектор оценок для каждой из меток $K$. Эти оценки представляют собой вероятности присутствия метки в следующем наборе, связанных с событием.

В нашей настройке мы ограничиваем размер прошлого, доступного модели.
$S^t = \{(X_{j}, Y_{j})\}_{j = t - \tau}^{t-1}$, где $\tau$ означает количество событий, предшествующих рассматриваемому событию, с отметкой времени $t$, которая равна приписывается целевому набору меток $Y_{t}$.
Итак, более формально $f(\cdot)$ имеет вид:
$$
f(X_{t - \tau}, \ldots, X_{t-1}, Y_{t - \tau}, \ldots, Y_{t-1}, \textbf{z}) \in [0, 1]^K
$$
для предсказания $Y_{t}$.

Чтобы завершить прогноз, нам нужна отдельная модель принятия решений о метках $g(f(\cdot))$, которая преобразует доверительные баллы в метки.
Например, мы сравниваем оценку для $k$-й метки с выбранным пороговым значением $\beta_k$: если $f_k(\cdot) > \beta_k, k = 1, \dots, K$, то модель предсказывает, что $k$-я метка присутствует. Таким образом, модель $g$ создает набор меток $\hat{Y}_{t} \subseteq \mathcal{Y}$ на основе входных оценок достоверности.

Результирующее качество модели зависит как от метода $g(\cdot)$ для выбора меток для конечного набора, так и от производительности $f(\cdot)$ для получения достоверных оценок, в то время как мы сосредоточены на работе с $f(\cdot)$. Далее мы преобразуем задачу классификации с несколькими метками в задачи множественной бинарной классификации и оптимизируем модель, минимизируя потери перекрестной энтропии.

\section{Архитектура LANET}
Большинство моделей, связанных с трансформаторами, используемых для последовательного предсказания с несколькими метками, используют вычисление собственного внимания между последовательными представлениями входных временных меток. 
Вместо этого LANET использует собственное внимание между представлениями меток. 
Итак, у него есть входные данные, состоящие из $K$ векторов.
Ниже мы опишем, как агрегировать последовательность векторов размером от $\tau$ до $K$ с помощью \textbf{Слоя векторных представлений}. Затем мы определяем \textbf{Слой собственного внимания}.
Чтобы получить предсказания, мы применяем \textbf{Слой предсказания}.

\paragraph{\textbf{Слой векторных представлений.}} Мы используем следующий подход для использования различных частей входных данных для последовательностей событий с несколькими метками:
\begin{itemize}
    \item \textbf{\emph{Векторное представление идентификаторов:}} Для идентификаторов мы изучаем матрицу эмбеддингов;
    \item \textbf{\emph{Векторное представление времени:}} Для каждой временной метки мы знаем значение $dt$, которое равно разнице в днях между рассматриваемой и предыдущей временной меткой. Мы обучаем представления для каждого наблюдаемого значения $dt$. Мы также учитываем порядок событий, поэтому мы смотрим на предстваления для позиций: $1, \dots,$ \textit{$\tau$}, чтобы добавить их к $dt$ представлению соответствующей временной метки; 
    \item \textbf{\emph{Представление суммы:}} Мы преобразуем все суммы в ячейки, разбивая непрерывные суммы сумм на интервалы. Каждому интервалу присваивается уникальный номер. Затем для каждого уникального номера мы создаем представление.
    \item \textbf{\emph{Transformer label encoder:}} Для построения входных данных LANET мы используем данные, связанные с определенным идентификатором для временных меток \textit{$\tau$}. Мы объединяем представление меток, которые встречаются во время временных меток \textit{$\tau$}, с соответствующими векторами времени и суммы. Если метка не принадлежит истории последних временных меток \textit{$\tau$}, мы добавляем к ней векторы нулей в виде представлений времени и суммы. Если конкретная метка встречается несколько раз на предыдущих шагах \textit{$\tau$}, то мы создаем временные представления для каждого отдельного вхождения, а затем суммируем их, чтобы получить окончательное представление времени для этой метки. Мы делаем то же самое при построении вектора суммы в ситуации повторения метки.
\end{itemize}

Итак, в результате слоя вложения у нас есть $K + 1$ векторов вложения. 
Первый вектор $\vecE_0$ соответствует вложениям общих (ID) признаков для последовательности.
Все остальные векторы $\vecE_k, k = 1, \dots, K$ являются объединением вложения метки с соответствующими вложениями времени и суммы. Оказывается, что векторы для исторически не задействованных меток --- это просто вложения меток, поскольку мы суммируем их с нулевыми векторами времени и количества просмотров.
При обучении всех весов встраивания они инициализируются из нормального распределения $\mathcal{N}(0, 1)$ и затем оптимизируются.
% Все остальные векторы являются конкатенациями вложения метки из словаря и означают объединения сумм и временных меток для конкретной метки.  

\paragraph{\textbf{Собственное внимание.}} После получения представлений из наших данных мы переходим к компоновке нашей архитектуры. 
Пусть $E = \{\vecE_0, \dots, \vecE_{K}\}$, $\vecE_i \in \mathbb{R}^d$ - последовательность входных представлений в энкодер, где $\vecE_0$ соответствует представлению $\mathbf{z}$ и все остальные $\vecE_i$ соответствуют представлениям, фиксирующим историческую информацию с точки зрения метки. 
В архитектуре Transformer влияние представление $\vecE_j$ на представление $\vecE_i$ достигается с помощью собственного внимания. Вес внимания $\alpha_{ij}$ и обновленный вектор $\vecE_i^{\prime}$ рассчитываются как:
$$ \vecE_i^{\prime} = \sum_{j = 0}^{K} \alpha_{ij} (W^v \vecE_j);\hspace{3 мм} \alpha_{ij} = \mathrm {softmax}\left ( (W ^ q \vecE_i)^T (W^k \vecE_j) / \sqrt{d} \right),$$ где $W^k$ --- весовая матрица ключа, $W^q$ - весовая матрица запроса, а $W^v$ - весовая матрица значения. Такую процедуру внедрения обновлений можно повторить несколько раз, указав количество уровней преобразователя-энкодера.
% Поверх подуровня самообучения мы применяем обычные процедуры уровня преобразователя-кодировщика~\cite{transformer_encoder_torch}.
% В частности, мы выполняем TODO.


\paragraph{\textbf{Уровень прогнозирования.}} Обновленные представления обрабатываются одним полносвязным слоем, чтобы получить окончательные представления для каждой метки $\{\vecE^{(final)}_j\}_{j = 1}^K$.
Таким образом, мы получаем $\{f_j\}_{j = 1}^K$, которые используются в классификаторе с несколькими метками с пороговым значением $t_j$, выбранным отдельно для каждой метки с использованием валидационной выборки.


\section{Планирование эксперимента}

\subsection{Описание данных}

\begin{table}[t!]
\centering
% \begin{tabular}{ccccccc}
\begin{tabular}{p{0.3cm}p{2.2cm}p{1.7cm}p{1.6cm}p{1.6cm}p{1.8cm}p{1.1cm}}
\hline
& Dataset & \# events & Median & Max & \# unique & Diff \\
& &  & set size & set size &  labels \\
\hline
& Sales & 47 217 & 16 & 48  & 84 & 0.0632 \\
& Demand & 5 912 & 13 & 24  & 33 & 0.0957 \\
\hline
\end{tabular}
\caption{Характеристики наборов данных, используемых в задачах последовательной классификации с несколькими метками.}
\label{tab:multi_datasets}
\end{table}

\textbf{Набор данных о продажах}~\cite{sales} - это исторические данные о продажах в разных магазинах. Метки относятся к категориям товаров, а сумма ---- это количество проданных товаров для определенной категории. 

\textbf{Набор данных о спросе}~\cite{demand} описывает исторический спрос на продукцию нескольких складов. Функция метки означает категорию продукта, а функция количества относится к соответствующему спросу.

Общая статистика по рассмотренным наборам данных приведена в таблице~\ref{tab:multi_datasets}.

Мы представляем количество наблюдаемых событий, средний размер набора всех доступных наборов меток $\mathrm{median}({|Y_{ij}|}_{i, j = 1, 1}^{n, t_i})$, максимальный размер набора меток, который встречается в набор данных $\max({|Y_{ij}|}_{i, j = 1, 1}^{n, t_i})$, количество уникальных меток $K$ и \textit{Diff}. 
\textit{Diff} измеряет дисбаланс меток: мы вычисляем 5$\%$ и 95$\%$ квантилей для частот меток и берем разницу между ними.
Представление меток не сбалансировано в большинстве наборов данных, но мы используем метрики, которые учитывают этот эффект.

\section{Основной эксперимент}
Эксперимент проводится на двух выборках Sales и Demand. Данные выборки представлены в таблице~\ref{tab:multi_datasets}. Основным результатом будет сравнение подхода  с собственным вниманием между представлениями на основе агрегации по меткам классов и подходом, использующим временную агрегацию для представлений. Также будут представлены базовые подходы, работающие с временными рядами это градиентный бустинг и LSTM. Сравнение представлено в таблице(тут будет таблица.).  

Стоит изучить, какой подход к вычислению собственного внимания более важен: между метками или между временными метками.
Таблица ... отвечает на этот вопрос.
Label-attention - это наша базовая реализация. Time-attention - это случай, когда мы учитываем внимание только между просмотрами временных меток. Concat-attention подразумевает получение оценок достоверности путем объединения label-attention и time-attention. Мы узнаем коэффициенты важности двух форм внимания и используем их в качестве весов при суммировании просмотров внимания в случае gated-attention. Эксперимент с индикацией отсутствия заключается в добавлении обучаемого вектора к входным вложениям, если конкретная метка не участвует в рассматриваемой истории.
тут какие-то выводы по таблице \ref{fig:fig2}
\begin{figure}
    \centering
    \includegraphics{images/fig2.png}
    \caption{Caption}
    \label{fig:fig2}
\end{figure}

Важным параметром нашей работы является ограничение исторической информации с помощью параметра $\tau$. В нашей работе мы концентрируемся на событиях с временными метками. Модель учитывает только последние временные метки для прогнозирования. Таким образом, возникает естественный вопрос, как качество модели зависит от количества временных меток, используемых для построения прогноза. Длина входной последовательности для модели равна параметру \textit{look\_back}. Зависимость показателя micro-AUC от \textit{look\_back} представлена в \ref{fig:fig1}

\begin{figure}
    \centering
    \includegraphics{images/fig1.png}
    \caption{Caption}
    \label{fig:fig1}
\end{figure}
 
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
